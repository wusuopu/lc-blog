<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: Scrapy | 龍昌博客]]></title>
  <link href="http://www.xefan.com/tags/Scrapy/atom.xml" rel="self"/>
  <link href="http://www.xefan.com/"/>
  <updated>2014-12-21T20:04:33+08:00</updated>
  <id>http://www.xefan.com/</id>
  <author>
    <name><![CDATA[龍昌]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Scrapy框架学习笔记3—— Scrapy与mongodb结合]]></title>
    <link href="http://www.xefan.com/archives/83890.html"/>
    <updated>2013-10-07T00:00:00+08:00</updated>
    <id>http://www.xefan.com/archives/83890-Scrapy框架学习笔记3—— Scrapy与mongodb结合</id>
    <content type="html"><![CDATA[<p>创建一个新的Item Pipeline，并将其添加到settings.py的ITEM_PIPELINES列表中。<br/>
在process_item方法中将item的数据保存到mongodb中。<br/>
scrapy的Item与dict相似，而mongodb中的数据是心bson格式保存的。因此Item的数据应该可以直接存储到mongodb中而几乎不用做额外的处理。</p>

<pre><code class="python">class MyMongoDBPipeline(object):
    def __init__(self, mongodb_server, mongodb_port, mongodb_db, mongodb_collection):
        connection = pymongo.Connection(mongodb_server, mongodb_port)
        self.mongodb_db = mongodb_db
        self.db = connection[mongodb_db]
        self.mongodb_collection = mongodb_collection
        self.collection = self.db[mongodb_collection]

    @classmethod
    def from_crawler(cls, crawler):
        # 连接mongodb
        return cls('localhost', 27017, 'scrapy', 'items')

    def process_item(self, item, spider):
        result = self.collection.insert(dict(item))
        log.msg("Item %s wrote to MongoDB database %s/%s" % (result, self.mongodb_db, self.mongodb_collection),
                level=log.DEBUG, spider=spider)
        return item
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scrapy框架学习笔记2—— Scrapy与Django结合]]></title>
    <link href="http://www.xefan.com/archives/83887.html"/>
    <updated>2013-10-07T00:00:00+08:00</updated>
    <id>http://www.xefan.com/archives/83887-Scrapy框架学习笔记2—— Scrapy与Django结合</id>
    <content type="html"><![CDATA[<p>前面也介绍过了Scrapy与Django的设计思想非常相似，因此这个两个结合也是比较容易的。<br/>
以下方法在Scrapy 0.18与Django 1.5下面测试是可以用的。</p>

<h3>1.首先设置Django的运行环境</h3>


<p>在settings.py中添加如下代码：</p>

<pre><code class="python">def setup_django_environment(path):
    import imp, os, sys
    from django.core.management import setup_environ
    m = imp.load_module('settings', *imp.find_module('settings', [path]))
    setup_environ(m)
    sys.path.append(os.path.abspath(os.path.join(path, os.path.pardir)))

setup_django_environment("/django/project/path")
</code></pre>

<p>注意：如果你的Django项目是用的sqlite数据库的话，那就需要设置为绝对路径，不能使用相对路径。</p>

<h3>2.创建django item</h3>


<p>首先在Django项目代码中创建一个Django的model，例如:</p>

<pre><code class="python">from django.db import models
class ScrapyModel(models.Model):
    title = models.CharField(max_length=200)
    link = models.CharField(max_length=200)
    desc = models.TextField()
</code></pre>

<p>然后在Scrapy项目中创建一个新的Item，只不过这次我们不再是继承自scrapy.item.Item，而是scrapy.contrib.djangoitem.DjangoItem:</p>

<pre><code class="python">class Test1DjItem(DjangoItem):
    django_model = ScrapyModel
</code></pre>

<p>用法与原来的Item相同，只是最后要执行一个save函数来调用django的save方法将数据存入数据库。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scrapy框架学习笔记1——概括]]></title>
    <link href="http://www.xefan.com/archives/83883.html"/>
    <updated>2013-10-07T00:00:00+08:00</updated>
    <id>http://www.xefan.com/archives/83883-Scrapy框架学习笔记1——概括</id>
    <content type="html"><![CDATA[<p>Scrapy是一个python的web爬虫框架，其设计思想与Django非常相似。如果事先用过Django的话那么理解Scrapy应该不难。</p>

<h3>0.安装Scrapy</h3>


<p>当前Scrapy的稳定版本是0.18<br/>
使用pip进行安装：  <code>pip install Scrapy</code><br/>
使用easy_install进行安装：  <code>easy_install Scrapy</code></p>

<p>初次使用可以按照它的官方教程操作，先熟悉一下：<br/>
<a href="http://doc.scrapy.org/en/0.18/intro/tutorial.html">http://doc.scrapy.org/en/0.18/intro/tutorial.html</a></p>

<h3>1.一个项目的基本流程</h3>


<p>  创建新项目  scrapy startproject &lt;name&gt;</p>

<p>  然后在&lt;name&gt;/spiders目录下新建一个python文件。<br/>
  编写一个新的class继承自 scrapy.spider.BaseSpider，并且需要设置如下属性：<br/>
    * name: 该爬虫的名字，不能与其它的相同<br/>
    * start_urls: 开始的url入口<br/>
    * parse(): 对从start_urls的获取的内容进行处理的函数，需要一个Response参数<br/>
    * allowed_domains: 一个列表，表示该爬虫允许爬取的网站域名。可选</p>

<p>  爬虫运行流程：<br/>
    * 首先调用 start_requests()方法访问start_urls的链接（默认的），然后由parse回调方法对请求的响应进行处理。<br/>
    * 在回调中处理Response，并返回Item或者Request，再或者是由这两种对象组成的一个可迭代的对象。<br/>
      Request对象对应的callback可以与parse相同，也可以不同。</p>

<p>  最后运行这个爬虫脚本：  scrapy crawl name</p>

<p>  也可以直接在终端运行一个选择器： scrapy shell &lt;url&gt;</p>

<h3>2.使用Item</h3>


<pre><code>scrapy.item.Item的调用接口类似于python的dict，Item包含多个scrapy.item.Field。  
这跟django的Model与Field有点相似。  
Item通常是在Spider的parse方法里使用，它用来保存解析到的数据。
</code></pre>

<h3>3.使用Item Pipeline</h3>


<p>  在settings.py中设置ITEM_PIPELINES，其默认为[]，与django的MIDDLEWARE_CLASSES等相似。<br/>
  从Spider的parse返回的Item数据将依次被ITEM_PIPELINES列表中的Pipeline类处理。</p>

<p>  一个Item Pipeline类必须实现以下方法：</p>

<blockquote><p> process_item(item, spider)</p>

<blockquote><p>为每个item pipeline组件调用，并且需要返回一个scrapy.item.Item实例对象或者抛出一个scrapy.exceptions.DropItem异常。<br/>
当抛出异常后该item将不会被之后的pipeline处理。<br/>
参数:<br/>
        item (Item object) – 由parse方法返回的Item对象<br/>
        spider (BaseSpider object) – 抓取到这个Item对象对应的爬虫对象</p></blockquote>

<p> 也可额外的实现以下两个方法：<br/>
 open_spider(spider)</p>

<blockquote><p>当爬虫打开之后被调用。<br/>
参数: spider (BaseSpider object) – 已经运行的爬虫</p></blockquote>

<p> close_spider(spider)</p>

<blockquote><p>当爬虫关闭之后被调用。<br/>
参数: spider (BaseSpider object) – 已经关闭的爬虫</p></blockquote></blockquote>

<h3>4.保存数据</h3>


<p>  想要保存抓取到的数据，最简单的方法是使用Feed exports。 参考官方文档：<a href="http://doc.scrapy.org/en/0.18/topics/feed-exports.html#topics-feed-exports">http://doc.scrapy.org/en/0.18/topics/feed-exports.html#topics-feed-exports</a></p>

<p>  例如将抓取的数据导出为json:  scrapy crawl dmoz -o items.json -t json</p>

<p>  对于小项目用这种方法也足够了。如果是比较复杂的数据的话可能就需要编写一个Item Pipeline进行处理了。</p>
]]></content>
  </entry>
  
</feed>
